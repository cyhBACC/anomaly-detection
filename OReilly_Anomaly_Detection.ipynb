{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read training data from CSV\n",
    "train_data_raw = pd.read_csv('resources/normal.csv')\n",
    "train_data_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read validation data from CSV\n",
    "validate_data_raw = pd.read_csv('resources/verify.csv')\n",
    "validate_data_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose only columns containing features we want to use for training\n",
    "feature_list = [\" LinAccX (g)\"]\n",
    "features = len(feature_list)\n",
    "train_data_selected = train_data_raw[feature_list].as_matrix().astype(np.float32)\n",
    "validate_data_selected = validate_data_raw[feature_list].as_matrix().astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.pylabtools import figsize\n",
    "figsize(16, 7)\n",
    "\n",
    "plt.plot(range(len(train_data_raw)), train_data_raw[feature_list], \"go\")\n",
    "plt.ylabel('LinAccX')\n",
    "plt.xlabel('Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training vs test mse\n",
    "def plot_errors(train_mse, test_mse):\n",
    "    epochs = range(len(train_mse))\n",
    "    figsize(16, 7)\n",
    "    plt.plot(epochs, train_mse, label='Train')\n",
    "    plt.plot(epochs, test_mse, label='Test')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi layer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a dataset into a windowed format - append *window - 1* records to the back of each record. \n",
    "# For example:\n",
    "# window = 2\n",
    "# dataset =\n",
    "# [\n",
    "#  [1, 2, 3],\n",
    "#  [2, 3, 4],\n",
    "#  [5, 6, 7]\n",
    "# ]\n",
    "#\n",
    "# Will result in:\n",
    "# [\n",
    "#  [0, 0, 0, 1, 2, 3], # padding with 0 since there were no observations before the first one\n",
    "#  [1, 2, 3, 2, 3, 4],\n",
    "#  [2, 3, 4, 5, 6, 7]\n",
    "# ]\n",
    "def prepare_dataset(dataset, window):\n",
    "    windowed_data = []\n",
    "    for i in range(len(dataset)):\n",
    "        start = i + 1 - window if i + 1 - window >= 0 else 0\n",
    "        observation = dataset[start : i + 1,]\n",
    "        to_pad = (window - i - 1 if i + 1 - window < 0 else 0) * features\n",
    "        observation = observation.flatten()\n",
    "        observation = np.lib.pad(observation, (to_pad, 0), 'constant', constant_values=(0, 0))\n",
    "        windowed_data.append(observation)\n",
    "    return np.array(windowed_data)\n",
    "    \n",
    "# Window size - the bigger this value is, the longer our network will \"retain\" past information i.e. it will take longer\n",
    "# for newer state to become significant but it will also mean we will use the time dependency factor more into account.\n",
    "window = 25\n",
    "\n",
    "# Convert our data into windowed formats\n",
    "data_train = prepare_dataset(train_data_selected, window)\n",
    "data_validate = prepare_dataset(validate_data_selected, window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "# When working with time-series data you usually should not do random samplic as temoporal dependency between observations\n",
    "# is of great important. The simples way to do a split in such a situation is just find a split point in the data\n",
    "# and use everything left to that point as training and the rest as test data.\n",
    "rows = len(data_train)\n",
    "split_factor = 0.8\n",
    "train = data_train[0:int(rows*split_factor)]\n",
    "test = data_train[int(rows*split_factor):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an iterator, which will feed our data in a batched fashion into MXNet\n",
    "batch_size = 256\n",
    "train_data = mx.gluon.data.DataLoader(train, batch_size, last_batch='keep', shuffle=False)\n",
    "test_data = mx.gluon.data.DataLoader(test, batch_size, last_batch='keep', shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.cpu() # use mx.cpu() on machines with no GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this network we will use our windowed data representation, so each record will actually contain all the features\n",
    "# from *window* number of records hence or total input size:\n",
    "total_features = window * features\n",
    "\n",
    "model = gluon.nn.Sequential()\n",
    "with model.name_scope():\n",
    "    # Connection between input and first hidden layer\n",
    "    model.add(gluon.nn.Dense(16, activation='tanh'))\n",
    "    model.add(gluon.nn.Dropout(0.25))\n",
    "\n",
    "    model.add(gluon.nn.Dense(8, activation='tanh'))\n",
    "    model.add(gluon.nn.Dropout(0.25))\n",
    "    \n",
    "    model.add(gluon.nn.Dense(16, activation='tanh'))\n",
    "    model.add(gluon.nn.Dropout(0.25))\n",
    "    \n",
    "    # Connection between hidden layer and our output layer - since it is an autoencoder we want the size to be same as input size\n",
    "    model.add(gluon.nn.Dense(total_features))\n",
    "\n",
    "# Use the non default Xavier parameter initializer\n",
    "model.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)\n",
    "\n",
    "# Use Adam optimizer for training\n",
    "trainer = gluon.Trainer(model.collect_params(), 'adam', {'learning_rate': 0.001})\n",
    "\n",
    "# Similarly to previous example we will use L2 loss for evaluation\n",
    "L = gluon.loss.L2Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_accuracy(data_iterator, model, L):\n",
    "    loss_avg = 0.\n",
    "    for i, data in enumerate(data_iterator):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = data\n",
    "        output = model(data)\n",
    "        loss = L(output, label)\n",
    "        loss_avg = loss_avg*i/(i+1) + nd.mean(loss).asscalar()/(i+1)\n",
    "    return loss_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "epochs = 60\n",
    "\n",
    "all_train_mse = []\n",
    "all_test_mse = []\n",
    "\n",
    "# Gluon training loop\n",
    "for e in range(epochs):\n",
    "    for i, data in enumerate(train_data):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = data\n",
    "        with autograd.record():\n",
    "            output = model(data)\n",
    "            loss = L(output, label)\n",
    "            loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "    \n",
    "    train_mse = evaluate_accuracy(train_data, model, L)\n",
    "    test_mse = evaluate_accuracy(test_data, model, L)\n",
    "    all_train_mse.append(train_mse)\n",
    "    all_test_mse.append(test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_errors(all_train_mse, all_test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper method running predictions in a batched fashion\n",
    "def predict(to_predict, L):\n",
    "    predictions = []\n",
    "    for i, data in enumerate(to_predict):\n",
    "        input = data.as_in_context(ctx)\n",
    "        out = model(input)\n",
    "        prediction = L(out, input).asnumpy().flatten()\n",
    "        predictions = np.append(predictions, prediction)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = mx.gluon.data.DataLoader(train, batch_size, shuffle=False)\n",
    "\n",
    "train_predictions = predict(train_data, L)\n",
    "threshold =  np.mean(train_predictions) + 3*np.std(train_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_data = mx.gluon.data.DataLoader(data_validate, batch_size, shuffle=False)\n",
    "\n",
    "validation_predictions = predict(validate_data, L)\n",
    "\n",
    "anomaly = list(map(lambda v: v > threshold, validation_predictions))\n",
    "\n",
    "# Result visualization\n",
    "anomaly = list(map(lambda v: \"red\" if v else \"green\", anomaly))\n",
    "figsize(16, 7)\n",
    "plt.scatter(list(range(len(validate_data_raw[\" LinAccX (g)\"]))), validate_data_raw[\" LinAccX (g)\"], c=anomaly)\n",
    "plt.ylabel('LinAccX')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long-short term memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd\n",
    "from mxnet import gluon\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup iterators for Gluon to feed our model with batched data during training\n",
    "batch_size = 128\n",
    "\n",
    "# Split the data into train and test\n",
    "rows = len(train_data_selected)\n",
    "split_factor = 0.8\n",
    "train = train_data_selected.astype(np.float32)[0:int(rows*split_factor)]\n",
    "test = train_data_selected.astype(np.float32)[int(rows*split_factor):]\n",
    "\n",
    "train_data = mx.gluon.data.DataLoader(train, batch_size, shuffle=False)\n",
    "test_data = mx.gluon.data.DataLoader(test, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.cpu() # use mx.cpu() on machines with no GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 1\n",
    "\n",
    "model = mx.gluon.nn.Sequential()\n",
    "with model.name_scope():\n",
    "    model.add(mx.gluon.rnn.LSTM(window, dropout=0.35))\n",
    "    model.add(mx.gluon.rnn.LSTM(features))\n",
    "\n",
    "# Use the non default Xavier parameter initializer\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx=ctx)\n",
    "\n",
    "# Use Adam optimizer for training\n",
    "trainer = gluon.Trainer(model.collect_params(), 'adam', {'learning_rate': 0.01})\n",
    "\n",
    "# Similarly to previous example we will use L2 loss for evaluation\n",
    "L = gluon.loss.L2Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_accuracy(data_iterator, model, L):\n",
    "    loss_avg = 0.\n",
    "    for i, data in enumerate(data_iterator):\n",
    "        data = data.as_in_context(ctx).reshape((-1, window, features, 1))\n",
    "        label = data\n",
    "        output = model(data)\n",
    "        loss = L(output, label)\n",
    "        loss_avg = loss_avg*i/(i+1) + nd.mean(loss).asscalar()/(i+1)\n",
    "    return loss_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "epochs = 25\n",
    "\n",
    "all_train_mse = []\n",
    "all_test_mse = []\n",
    "\n",
    "# Gluon training loop\n",
    "for e in range(epochs):\n",
    "    for i, data in enumerate(train_data):\n",
    "        data = data.as_in_context(ctx).reshape((-1, window, features,1))\n",
    "        label = data\n",
    "        with autograd.record():\n",
    "            output = model(data)\n",
    "            loss = L(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(batch_size)\n",
    "    \n",
    "    train_mse = evaluate_accuracy(train_data, model, L)\n",
    "    test_mse = evaluate_accuracy(test_data, model, L)\n",
    "    all_train_mse.append(train_mse)\n",
    "    all_test_mse.append(test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_errors(all_train_mse, all_test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper method running predictions in a batched fashion\n",
    "def predict(to_predict, L):\n",
    "    predictions = []\n",
    "    for i, data in enumerate(to_predict):\n",
    "        input = data.as_in_context(ctx).reshape((-1,features,1))\n",
    "        out = model(input)\n",
    "        prediction = L(out, input).asnumpy().flatten()\n",
    "        predictions = np.append(predictions, prediction)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = mx.gluon.data.DataLoader(train_data_selected.astype(np.float32), batch_size, shuffle=False)\n",
    "\n",
    "train_predictions = predict(train_data, L)\n",
    "threshold =  np.mean(train_predictions) + 3*np.std(train_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_data = mx.gluon.data.DataLoader(validate_data_selected.astype(np.float32), batch_size, shuffle=False)\n",
    "\n",
    "validation_predictions = predict(validate_data, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly = list(map(lambda v: v > threshold, validation_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result visualization\n",
    "anomaly = list(map(lambda v: \"red\" if v else \"green\", anomaly))\n",
    "figsize(16, 7)\n",
    "plt.scatter(list(range(len(validate_data_raw[\" LinAccX (g)\"]))), validate_data_raw[\" LinAccX (g)\"], c=anomaly)\n",
    "plt.ylabel('LinAccX')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-anomaly",
   "language": "python",
   "name": "oreilly-anomaly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
